---
title: "practica 3"
format: pdf
editor: visual
jupyter: python3
---

```{=tex}
\tableofcontents
\newpage
\section{correlaci√≥n lineal}
```

# **correlaci√≥n lineal**

Un estudio pretende analizar si existe una correlaci√≥n lineal positiva entre la altura y el peso de las personas. Los datos utilizados en este ejemplo se han obtenido del libro *Statistical Rethinking by Richard McElreath*. El set de datos contiene informaci√≥n recogida por Nancy Howell a finales de la d√©cada de 1960 sobre el pueblo !Kung San, que viven en el desierto de Kalahari entre Botsuana, Namibia y Angola.

## librerias:

Tratamiento de datos

```{python}


import pandas as pd
import numpy as np
```

Gr√°ficos

```{python}


import matplotlib.pyplot as plt
from matplotlib import style
import seaborn as sns
```

Preprocesado y an√°lisis

```{python}


import statsmodels.api as sm
import pingouin as pg
from scipy import stats
from scipy.stats import pearsonr
```

Configuraci√≥n matplotlib y Configuraci√≥n warnings

```{python}

plt.style.use('ggplot')

# Configuraci√≥n warnings
import warnings
warnings.filterwarnings('ignore')
```

## datos

```{python}
url = ('https://raw.githubusercontent.com/JoaquinAmatRodrigo/' +
       'Estadistica-machine-learning-python/master/data/Howell1.csv')
datos = pd.read_csv(url)
```

Se utilizan √∫nicamente informaci√≥n de individuos mayores de 18 a√±os.

```{python}

datos = datos[datos.age > 18]

datos.info()
```

En primer lugar se representan las dos variables mediante un diagrama de dispersi√≥n (*scatterplot*) para intuir si existe relaci√≥n lineal o monot√≥nica. Si no la hay, no tiene sentido calcular este tipo de correlaciones.

### graficos

```{python}
fig, ax = plt.subplots(1, 1, figsize=(6,4))
ax.scatter(x=datos.height, y=datos.weight, alpha= 0.8)
ax.set_xlabel('Altura')
ax.set_ylabel('Peso');
```

El diagrama de dispersi√≥n parece indicar una relaci√≥n lineal positiva entre ambas variables.

Para poder elegir el coeficiente de correlaci√≥n adecuado, se tiene que analizar el tipo de variables y la distribuci√≥n que presentan. En este caso, ambas variables son cuantitativas continuas y pueden ordenarse para convertirlas en un ranking, por lo que, a priori, los tres coeficientes podr√≠an aplicarse. La elecci√≥n se har√° en funci√≥n de la distribuci√≥n que presenten las observaciones: normalidad, homocedasticidad y presencia de *outliers*.

### **Normalidad**

Gr√°fico distribuci√≥n variables

```{python}


fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))

axs[0].hist(x=datos.height, bins=20, color="#3182bd", alpha=0.5)
axs[0].plot(datos.height, np.full_like(datos.height, -0.01), '|k', markeredgewidth=1)
axs[0].set_title('Distribuci√≥n altura (height)')
axs[0].set_xlabel('height')
axs[0].set_ylabel('counts')

axs[1].hist(x=datos.weight, bins=20, color="#3182bd", alpha=0.5)
axs[1].plot(datos.weight, np.full_like(datos.weight, -0.01), '|k', markeredgewidth=1)
axs[1].set_title('Distribuci√≥n peso (weight)')
axs[1].set_xlabel('weight')
axs[1].set_ylabel('counts')


plt.tight_layout();
```

Gr√°fico Q-Q

```{python}


fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))

sm.qqplot(
    datos.height,
    fit   = True,
    line  = 'q',
    alpha = 0.4,
    lw    = 2,
    ax    = axs[0]
)
axs[0].set_title('Gr√°fico Q-Q height', fontsize = 10, fontweight = "bold")
axs[0].tick_params(labelsize = 7)

sm.qqplot(
    datos.height,
    fit   = True,
    line  = 'q',
    alpha = 0.4,
    lw    = 2,
    ax    = axs[1]
)
axs[1].set_title('Gr√°fico Q-Q height', fontsize = 10, fontweight = "bold")
axs[1].tick_params(labelsize = 7)
```

Adem√°s del estudio gr√°fico, se recurre a dos test estad√≠sticos que contrasten la normalidad de los datos: *Shapiro-Wilk test* y *D'Agostino's K-squared test*. Este √∫ltimo es el que incluye el *summary* de **statsmodels** bajo el nombre de *Omnibus*.

En ambos test, la hip√≥tesis nula considera que los datos siguen una distribuci√≥n normal, por lo tanto, si el *p-value* no es inferior al nivel de referencia *alpha* seleccionado, no hay evidencias para descartar que los datos se distribuyen de forma normal.

Normalidad de los residuos Shapiro-Wilk test

```{python}


shapiro_test = stats.shapiro(datos.height)
print(f"Variable height: {shapiro_test}")
shapiro_test = stats.shapiro(datos.weight)
print(f"Variable weight: {shapiro_test}")
```

Normalidad de los residuos D'Agostino's K-squared test

```{python}


k2, p_value = stats.normaltest(datos.height)
print(f"Variable height: Estad√≠tico = {k2}, p-value = {p_value}")
k2, p_value = stats.normaltest(datos.weight)
print(f"Variable weight: Estad√≠tico = {k2}, p-value = {p_value}")
```

El an√°lisis gr√°fico y los test estad√≠sticos muestran evidencias de que no se puede asumir normalidad en ninguna de las dos variables. Siendo estrictos, este hecho excluye la posibilidad de utilizar el coeficiente de Pearson, dejando como alternativas el de Spearman o Kendall. Sin embargo, dado que la distribuci√≥n no se aleja mucho de la normalidad y de que el coeficiente de Pearson tiene cierta robustez, a fines pr√°cticos s√≠ que se podr√≠a utilizar siempre y cuando se tenga en cuenta este hecho y se comunique en los resultados. Otra posibilidad es tratar de transformar las variables para mejorar su distribuci√≥n, por ejemplo, aplicando el logaritmo.

Transformaci√≥n logar√≠tmica de los datos

```{python}


fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))

sm.qqplot(
    np.log(datos.height),
    fit   = True,
    line  = 'q',
    alpha = 0.4,
    lw    = 2,
    ax    = ax
)
ax.set_title('Gr√°fico Q-Q log(height)', fontsize = 13)
ax.tick_params(labelsize = 7)


shapiro_test = stats.shapiro(np.log(datos.height))
print(f"Variable height: {shapiro_test}")
```

La trasformaci√≥n logar√≠tmica de la variable altura (*height*) consigue una distribuci√≥n m√°s pr√≥xima a la normal.

### **Homocedasticidad**

La homocedasticidad implica que la varianza se mantiene constante. Puede analizarse de forma gr√°fica representando las observaciones en un diagrama de dispersi√≥n y viendo si mantiene una homogeneidad en su dispersi√≥n a lo largo del eje X. Una forma c√≥nica es un claro indicativo de falta de homocedasticidad. Dos test estad√≠sticos utilizados para contrastar la homocedasticidad son: test de *Goldfeld-Quandt* y el de *Breusch-Pagan*.

Tal como muestra el diagrama de dispersi√≥n generado al inicio del ejercicio, no se aprecia ning√∫n patr√≥n c√≥nico y la dispersi√≥n es constante.

### **Coeficientes correlaci√≥n**

Debido a la falta de normalidad, los resultados generados por Pearson no son del todo precisos. Sin embargo, dado que la desviaci√≥n de la normalidad es leve y no se aprecian *outliers*, con fines ilustrativos, se procede a calcular los tres tipos de coeficientes.

De nuevo recordar que, cuando alguna de las condiciones asumidas por un modelo o test estad√≠stico no se cumplen, no significa que obligatoriamente se tenga que descartar, pero hay que ser consciente de las implicaciones que tiene y reportarlo siempre en los resultados.

### Pandas

Pandas permite calcular la correlaci√≥n de dos Series (columnas de un DataFrame). El c√°lculo se hace por pares, eliminando autom√°ticamente aquellos con valores NA/null. Una limitaci√≥n de Pandas es que no calcula la significancia estad√≠stica.

C√°lculo de correlaci√≥n con Pandas

```{python}

print('Correlaci√≥n Pearson: ', datos['weight'].corr(datos['height'], method='pearson'))
print('Correlaci√≥n spearman: ', datos['weight'].corr(datos['height'], method='spearman'))
print('Correlaci√≥n kendall: ', datos['weight'].corr(datos['height'], method='kendall'))
```

**Scypy.stats**

La implementaci√≥n de Scypy.stats s√≠ permite calcular la significancia estad√≠stica adem√°s del coeficiente de correlaci√≥n. La funci√≥n `stats.pearsonr()`, devuelve un error si alguna de las observaciones contienen valores NA/null. Las funciones `stats.spearmanr()` y `stats.kendalltau()` s√≠ permiten excluirlos de forma autom√°tica si se indica `nan_policy='omit'`.

C√°lculo de correlaci√≥n y significancia con Scipy

```{python}


r, p = stats.pearsonr(datos['weight'], datos['height'])
print(f"Correlaci√≥n Pearson: r={r}, p-value={p}")

r, p = stats.spearmanr(datos['weight'], datos['height'])
print(f"Correlaci√≥n Spearman: r={r}, p-value={p}")

r, p = stats.kendalltau(datos['weight'], datos['height'])
print(f"Correlaci√≥n Pearson: r={r}, p-value={p}")

```

**Pingouin**

La librer√≠a [Pingouin](https://pingouin-stats.org/) tiene una de las implementaciones m√°s completas. Con la funci√≥n `corr()` se obtiene, adem√°s del coeficiente de correlaci√≥n, su significancia, intervalo de confianza y poder estad√≠stico entre otros.

C√°lculo de correlaci√≥n, significancia e intervalos con pingouin

```{python}

display(pg.corr(datos['weight'], datos['height'], method='pearson'))
display(pg.corr(datos['weight'], datos['height'], method='spearman'))
display(pg.corr(datos['weight'], datos['height'], method='kendall'))
```

### **Conclusi√≥n**

Los test estad√≠sticos muestran una correlaci√≥n lineal entre moderada y alta, con claras evidencias estad√≠sticas de que la relaci√≥n observada no se debe al azar (pvalue‚âà0ùëùùë£ùëéùëôùë¢ùëí‚âà0).
